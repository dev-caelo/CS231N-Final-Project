{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f9d119-1c9c-4d1b-923b-e6bd49f45b5f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Hello! Welcome to the architecture setup notebook, where we will be installing all requirements and outline the basic architecture of our AlexNet model (whose performance will be compared to our custom model, EfficentNet, and ConvNeXt). \n",
    "\n",
    "\n",
    "The cell below handles our initial requirements installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e7b0f3-17de-4abc-bdd0-ea94640db448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (from -r ../../requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.8/site-packages (from -r ../../requirements.txt (line 2)) (4.41.2)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.8/site-packages (from -r ../../requirements.txt (line 3)) (3.7.5)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.8/site-packages (from -r ../../requirements.txt (line 4)) (2.3.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.18.0-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "Requirement already satisfied: kaggle in /home/ubuntu/.local/lib/python3.8/site-packages (from -r ../../requirements.txt (line 6)) (1.6.14)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "Collecting keras\n",
      "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.82-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers->-r ../../requirements.txt (line 2)) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers->-r ../../requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers->-r ../../requirements.txt (line 2)) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r ../../requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (4.53.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r ../../requirements.txt (line 3)) (6.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (3.1)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch->-r ../../requirements.txt (line 4)) (4.12.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.3.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->-r ../../requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from kaggle->-r ../../requirements.txt (line 6)) (6.1.0)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle->-r ../../requirements.txt (line 6)) (1.25.8)\n",
      "Requirement already satisfied: python-slugify in /home/ubuntu/.local/lib/python3.8/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (8.0.4)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle->-r ../../requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /home/ubuntu/.local/lib/python3.8/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (2024.6.2)\n",
      "Collecting scipy>=1.5.0\n",
      "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.64.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow->-r ../../requirements.txt (line 8)) (45.2.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r ../../requirements.txt (line 11)) (2024.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->-r ../../requirements.txt (line 2)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->-r ../../requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/ubuntu/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib->-r ../../requirements.txt (line 3)) (3.19.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from sympy->torch->-r ../../requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->-r ../../requirements.txt (line 4)) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->-r ../../requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->kaggle->-r ../../requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from python-slugify->kaggle->-r ../../requirements.txt (line 6)) (1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow->-r ../../requirements.txt (line 8)) (0.34.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r ../../requirements.txt (line 8)) (0.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow->-r ../../requirements.txt (line 8)) (7.1.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r ../../requirements.txt (line 8)) (0.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->-r ../../requirements.txt (line 8)) (3.1.0)\n",
      "\u001b[31mERROR: tensorflow 2.13.1 has requirement keras<2.14,>=2.13.1, but you'll have keras 2.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.13.1 has requirement numpy<=1.24.3,>=1.22, but you'll have numpy 1.24.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.13.1 has requirement typing-extensions<4.6.0,>=3.6.6, but you'll have typing-extensions 4.12.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torchvision, scipy, joblib, threadpoolctl, scikit-learn, gast, grpcio, astunparse, protobuf, tensorflow-estimator, h5py, termcolor, libclang, wrapt, absl-py, flatbuffers, opt-einsum, tensorflow-io-gcs-filesystem, google-pasta, rsa, cachetools, google-auth, markdown, tensorboard-data-server, werkzeug, requests-oauthlib, google-auth-oauthlib, tensorboard, keras, tensorflow, tzdata, pandas, seaborn, opencv-python\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.29.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 joblib-1.4.2 keras-2.15.0 libclang-18.1.1 markdown-3.6 opencv-python-4.10.0.82 opt-einsum-3.3.0 pandas-2.0.3 protobuf-4.25.3 requests-oauthlib-2.0.0 rsa-4.9 scikit-learn-1.3.2 scipy-1.10.1 seaborn-0.13.2 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 threadpoolctl-3.5.0 torchvision-0.18.0 tzdata-2024.1 werkzeug-3.0.3 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d52aad-b30b-493d-ba92-5037af2e7f0a",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As part of our data preprocessing, we will split the down-scaled lung dataset from the original dataset into a train/test split. \n",
    "\n",
    "Note that we will be using five-fold cross-validation for testing later, hence we will not be partioning an additional validation set. \n",
    "\n",
    "After splitting our data, we will then feed the training set into our models. Here, we will specifically feed it into the AlexNet model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b162f0-bf43-4f54-b158-99fea42dffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import os\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "869c1863-6fc2-42b3-ae1a-aad7024bfdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded AlexNet SVM classifier into svm_tester_classifier\n",
      "Could not import AlexNet Softmax classifier, file may not exist.\n",
      "Could not import AlexNet SVM+PCA classifier, file may not exist.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved pipelines from training files\n",
    "try:\n",
    "    svm_tester_classifier = joblib.load('alexnet_svm.pkl')\n",
    "    print('Loaded AlexNet SVM classifier into svm_tester_classifier')\n",
    "except:\n",
    "    print('Could not import AlexNet SVM classifier, file may not exist.', )\n",
    "\n",
    "try:\n",
    "    softmax_tester_classifier = joblib.load('alexnet_softmax.pkl')\n",
    "    print('Loaded AlexNet Softmax classifier into softmax_tester_classifier')\n",
    "except:\n",
    "    print('Could not import AlexNet Softmax classifier, file may not exist.')\n",
    "\n",
    "try:\n",
    "    pca_tester_classifier = joblib.load('alexnet_pca.pkl')\n",
    "    print('Loaded AlexNet SVM+PCA classifier into pca_tester_classifier')\n",
    "except:\n",
    "    print('Could not import AlexNet SVM+PCA classifier, file may not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733a84b-4a8a-467a-a710-00481ee33691",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The code below extracts images from our dataset, resizes each into a fourth their original size (768 -> 192), and converts them into Torch tensors. The ImageFolder class allows us to lazyload our images to preserve our computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b130d35-ed66-48f9-8bc4-70d63be800f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to our lung_image_sets\n",
    "data_dir = \"../../lung_colon_image_set/lung_image_sets\"\n",
    "\n",
    "# Define resized size of images (Put this back to 192 later, recommended size of 224)\n",
    "resized_size = 100\n",
    "\n",
    "# Convert images into Tensors\n",
    "tensor_data = transforms.Compose([\n",
    "  transforms.Resize((resized_size, resized_size)),   # Cut image into a fourth of original size\n",
    "  transforms.Grayscale(num_output_channels=1),      # Convert to grayscale\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Lambda(lambda x: x.repeat(3, 1, 1))    # Duplicate channels to get 3 channel image\n",
    "])\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "data = ImageFolder(root=data_dir, transform=tensor_data)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train, test = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "load_train = DataLoader(train, batch_size=32, shuffle=True)\n",
    "load_test = DataLoader(test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493416c-19f2-4ec5-b0fc-55087b8258bd",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "We will initialize the AlexNet model using Pytorch's pretrained AlexNet model and remove the final layer to perform feature extraction on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5eb6da-8330-44ec-96d5-ead70efe5b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100%|█████████████████████████████████████████| 233M/233M [00:01<00:00, 228MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize AlexNet Model \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# Modify AlexNet to Extract Features\n",
    "# Note: we are removing the final layer\n",
    "model = torch.nn.Sequential(*list(alexnet.children())[:-1])\n",
    "model.eval()\n",
    "model = model\n",
    "num_features = alexnet.classifier[6].in_features\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 5e-4\n",
    "momentum = 0.9\n",
    "\n",
    "# Define our loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c870c-0c29-479f-9ac4-7e24f497bf26",
   "metadata": {},
   "source": [
    "## AlexNet + SVM Classifier Training and Testing\n",
    "We will perform k-fold cross-validation testing on the SVM classifier, which is trained the on features extracted by our AlexNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86ef1e2-752f-4e85-bedd-1e3508f3e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for training and validation sets\n",
    "def extract_features(loader, model):\n",
    "    features_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs\n",
    "            labels = labels\n",
    "            features = model(inputs)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    return np.concatenate(features_list), np.concatenate(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb72148-5f67-42da-89be-47f3d424677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 1 Accuracy: 0.8068\n",
      "Fold 2\n",
      "Fold 2 Accuracy: 0.8183\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 2 FOLDS\n",
      "--------------------------------\n",
      "Fold 1: 0.8068\n",
      "Fold 2: 0.8183\n",
      "Average: 0.8126\n"
     ]
    }
   ],
   "source": [
    "# Store the results of each fold\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=231)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Create the SVM classifier\n",
    "svm_classifier = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train), 1):\n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    # Create data samplers for train and validation sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create data loaders for train and validation sets\n",
    "    train_loader = DataLoader(train, batch_size=32, sampler=train_sampler)\n",
    "    val_loader = DataLoader(train, batch_size=32, sampler=val_sampler)\n",
    "    \n",
    "    # Extract features for training and validation sets\n",
    "    train_features, train_labels = extract_features(train_loader, model)\n",
    "    val_features, val_labels = extract_features(val_loader, model)\n",
    "\n",
    "    # Train the SVM classifier\n",
    "    svm_classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Evaluate the classifier on the validation set\n",
    "    val_predictions = svm_classifier.predict(val_features)\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    results[fold] = accuracy\n",
    "    print(f'Fold {fold} Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print the average accuracy across all folds\n",
    "average_accuracy = np.mean(list(results.values()))\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "for fold in results:\n",
    "    print(f'Fold {fold}: {results[fold]:.4f}')\n",
    "print(f'Average: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b30e26-ca10-41c9-ab91-5308cb4a7cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet_svm.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to save our model into a pickle file\n",
    "joblib.dump(svm_classifier, 'alexnet_svm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c0991-e26b-4c2c-85da-e4c7c60eb54c",
   "metadata": {},
   "source": [
    "## AlexNet + Softmax Classifier Training and Testing\n",
    "We will perform k-fold cross-validation testing on the Softmax classifier, which is trained the on features extracted by our AlexNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4b6ff12-1f1c-490c-9aab-6071a18deb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolegarcia/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.9160\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolegarcia/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Accuracy: 0.9125\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 2 FOLDS\n",
      "--------------------------------\n",
      "Fold 1: 0.9160\n",
      "Fold 2: 0.9125\n",
      "Average: 0.9143\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "softmax_classifier = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(multi_class='multinomial', solver='lbfgs', C=1.0, random_state=231, max_iter=400)\n",
    ")\n",
    "\n",
    "# Store the results of each fold\n",
    "results = {}\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=231)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train), 1):\n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    # Create data samplers for train and validation sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create data loaders for train and validation sets\n",
    "    train_loader = DataLoader(train, batch_size=32, sampler=train_sampler)\n",
    "    val_loader = DataLoader(train, batch_size=32, sampler=val_sampler)\n",
    "    \n",
    "    # Extract features for training and validation sets\n",
    "    train_features, train_labels = extract_features(train_loader, model)\n",
    "    val_features, val_labels = extract_features(val_loader, model)\n",
    "\n",
    "    # Train the Softmax classifier\n",
    "    softmax_classifier.fit(train_features, train_labels)\n",
    "    \n",
    "    # Evaluate the classifier on the validation set and extract metrics\n",
    "    val_predictions = softmax_classifier.predict(val_features)\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    \n",
    "    results[fold] = accuracy\n",
    "    print(f'Fold {fold} Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print the average accuracy across all folds\n",
    "average_accuracy = np.mean(list(results.values()))\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "for fold in results:\n",
    "    print(f'Fold {fold}: {results[fold]:.4f}')\n",
    "print(f'Average: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215aae4b-1935-4ab9-88c2-7e73cfebe2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to save our model into a pickle file\n",
    "joblib.dump(softmax_classifier, 'alexnet_softmax.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12377846-c63a-41c8-923d-3902e581801e",
   "metadata": {},
   "source": [
    "## AlexNet + PCA + SVM Classifier Training and Testing\n",
    "As an extension to our SVM implementation, the paper suggests that applying PCA on the resulting features derives higher accuracy before being loaded into the SVM classifier. We implement this approach below, performing k-fold cross-validation testing on the PCA + SVM classifier, which is trained the on features extracted by our AlexNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed75428c-079a-45ec-836f-7ef682d91427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 1 Accuracy: 0.8878\n",
      "Fold 2\n",
      "Fold 2 Accuracy: 0.8823\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 2 FOLDS\n",
      "--------------------------------\n",
      "Fold 1: 0.8878\n",
      "Fold 2: 0.8823\n",
      "Average: 0.8851\n"
     ]
    }
   ],
   "source": [
    "# Store the results of each fold\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=231)\n",
    "results = {}\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "n_components = 24  # Set the number of principal components you want to keep\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Create the SVM classifier\n",
    "svm_pca_classifier = make_pipeline(StandardScaler(), pca, SVC(kernel='linear'))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train), 1):\n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    # Create data samplers for train and validation sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create data loaders for train and validation sets\n",
    "    train_loader = DataLoader(train, batch_size=32, sampler=train_sampler)\n",
    "    val_loader = DataLoader(train, batch_size=32, sampler=val_sampler)\n",
    "\n",
    "    # Extract features for training and validation sets\n",
    "    train_features, train_labels = extract_features(train_loader, model)\n",
    "    val_features, val_labels = extract_features(val_loader, model)\n",
    "\n",
    "    # Train the SVM classifier with PCA\n",
    "    svm_pca_classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Evaluate the classifier on the validation set\n",
    "    val_predictions = svm_pca_classifier.predict(val_features)\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    results[fold] = accuracy\n",
    "    print(f'Fold {fold} Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print the average accuracy across all folds\n",
    "average_accuracy = np.mean(list(results.values()))\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "for fold in results:\n",
    "    print(f'Fold {fold}: {results[fold]:.4f}')\n",
    "print(f'Average: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c38b48-b8e8-4202-856c-93f8e6cf0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to save our model into a pickle file\n",
    "joblib.dump(svm_pca_classifier, 'alexnet_pca.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329dde2e-bad8-4e08-a89e-e92e765d16b0",
   "metadata": {},
   "source": [
    "### Testing and Metrics\n",
    "\n",
    "Now with our trained models, we will now test with our test set and store metrics for each model. The metrics that we will store are the following:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "\n",
    "The metrics are defined in our paper more clearly, but to calculate these we will calculate the the following values:\n",
    "- True Positive (TP)\n",
    "- False Positive (FP)\n",
    "- True Negative (TN)\n",
    "- False Negative (FN)\n",
    "\n",
    "We calculate these values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d36dea-65a1-4a1c-a899-44a76bb9d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Accuracy: 0.8163, Precision: 0.8177, Recall: 0.8163, F1 Score: 0.8169\n",
      "SVM tester - Accuracy: 0.8163, Precision: 0.8177, Recall: 0.8163, F1 Score: 0.8169\n",
      "Comparison of Classifiers on Test Set:\n",
      "SVM: Accuracy=0.8163, Precision=0.8177, Recall=0.8163, F1 Score=0.8169\n",
      "SVM tester: Accuracy=0.8163, Precision=0.8177, Recall=0.8163, F1 Score=0.8169\n"
     ]
    }
   ],
   "source": [
    "# Extract Features\n",
    "test_features, test_labels = extract_features(load_test, model)\n",
    "\n",
    "# List of trained classifiers\n",
    "classifiers = {\n",
    "    'SVM': svm_classifier,   # Assume svm_model is already trained\n",
    "    # 'SVM tester': svm_tester_classifier\n",
    "    'Softmax': softmax_classifier,  # Another trained classifier\n",
    "    'SVM+PCA': svm_pca_classifier   # Another trained classifier\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {clf_name: {} for clf_name in classifiers}\n",
    "\n",
    "# Evaluate each classifier\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Predict using the classifier\n",
    "    test_predictions = clf.predict(test_features)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "    recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "    f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "    \n",
    "    # Store the results\n",
    "    results[clf_name]['accuracy'] = accuracy\n",
    "    results[clf_name]['precision'] = precision\n",
    "    results[clf_name]['recall'] = recall\n",
    "    results[clf_name]['f1'] = f1\n",
    "\n",
    "    # Print the results\n",
    "    print(f'{clf_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "# Print a summary of the results\n",
    "print('Comparison of Classifiers on Test Set:')\n",
    "for clf_name in results:\n",
    "    print(f'{clf_name}: Accuracy={results[clf_name][\"accuracy\"]:.4f}, Precision={results[clf_name][\"precision\"]:.4f}, Recall={results[clf_name][\"recall\"]:.4f}, F1 Score={results[clf_name][\"f1\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55df77-efc0-465c-992a-7faf7887b992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
