{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f9d119-1c9c-4d1b-923b-e6bd49f45b5f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Hello! Welcome to the architecture setup notebook, where we will be installing all requirements and outline the basic architecture of our AlexNet model (whose performance will be compared to our custom model, EfficentNet, and ConvNeXt). \n",
    "\n",
    "\n",
    "The cell below handles our initial requirements installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e7b0f3-17de-4abc-bdd0-ea94640db448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: transformers in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 2)) (4.41.1)\n",
      "Requirement already satisfied: matplotlib in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 3)) (3.7.2)\n",
      "Requirement already satisfied: torch in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: torchvision in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 5)) (0.17.0)\n",
      "Requirement already satisfied: kaggle in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 6)) (1.6.14)\n",
      "Requirement already satisfied: scikit-learn in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: tensorflow in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: keras in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 9)) (2.12.0)\n",
      "Requirement already satisfied: seaborn in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: pandas in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 11)) (2.0.3)\n",
      "Requirement already satisfied: opencv-python in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from -r ../../requirements.txt (line 13)) (4.9.0.80)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: filelock in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from transformers->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (4.25.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (6.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from torch->-r ../../requirements.txt (line 4)) (2024.5.0)\n",
      "Requirement already satisfied: urllib3 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (1.26.18)\n",
      "Requirement already satisfied: six>=1.10 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (2024.2.2)\n",
      "Requirement already satisfied: python-slugify in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (8.0.4)\n",
      "Requirement already satisfied: bleach in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (6.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from scikit-learn->-r ../../requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from scikit-learn->-r ../../requirements.txt (line 7)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from scikit-learn->-r ../../requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (0.34.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (18.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (1.64.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (24.3.25)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (2.12.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (65.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (3.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (0.4.13)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from pandas->-r ../../requirements.txt (line 11)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from pandas->-r ../../requirements.txt (line 11)) (2023.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow->-r ../../requirements.txt (line 8)) (0.37.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->-r ../../requirements.txt (line 3)) (3.17.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow->-r ../../requirements.txt (line 8)) (7.1.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow->-r ../../requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (3.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from requests->transformers->-r ../../requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from requests->transformers->-r ../../requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: webencodings in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from bleach->kaggle->-r ../../requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from jinja2->torch->-r ../../requirements.txt (line 4)) (2.1.3)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from python-slugify->kaggle->-r ../../requirements.txt (line 6)) (1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from sympy->torch->-r ../../requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (5.3.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/nicolegarcia/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r ../../requirements.txt (line 8)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d52aad-b30b-493d-ba92-5037af2e7f0a",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As part of our data preprocessing, we will split the down-scaled lung dataset from the original dataset into a train/test split. \n",
    "\n",
    "Note that we will be using five-fold cross-validation for testing later, hence we will not be partioning an additional validation set. \n",
    "\n",
    "After splitting our data, we will then feed the training set into our models. Here, we will specifically feed it into the AlexNet model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b162f0-bf43-4f54-b158-99fea42dffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import os\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733a84b-4a8a-467a-a710-00481ee33691",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The code below extracts images from our dataset, resizes each into a fourth their original size (768 -> 192), and converts them into Torch tensors. The ImageFolder class allows us to lazyload our images to preserve our computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b130d35-ed66-48f9-8bc4-70d63be800f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path to our lung_image_sets\n",
    "data_dir = \"../../lung_colon_image_set/lung_image_sets\"\n",
    "\n",
    "# Define resized size of images (Put this back to 192 later, recommended size of 224)\n",
    "resized_size = 224\n",
    "\n",
    "# Convert images into Tensors\n",
    "tensor_data = transforms.Compose([\n",
    "  transforms.Resize((resized_size, resized_size)),   # Cut image into a fourth of original size\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "data = ImageFolder(root=data_dir, transform=tensor_data)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train, test = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "# Create KFold object with 5 folds\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=231)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493416c-19f2-4ec5-b0fc-55087b8258bd",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "We will initialize the AlexNet model using Pytorch's Torchvision pretrained ConvNeXt model and remove the final layer to perform feature extraction on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a3fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import convnext_base\n",
    "\n",
    "# Define the ConvNeXtCNN model\n",
    "class ConvNeXtCNN(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(ConvNeXtCNN, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ConvNeXt model as our feature extractor\n",
    "        self.convnext = convnext_base(pretrained=True)\n",
    "        \n",
    "        # Replace the last layer with an Identity matrix (essentially removes the last FC layer)\n",
    "        self.convnext.classifier = nn.Identity()\n",
    "\n",
    "        # Freeze our ConvNeXt model\n",
    "        self.convnext.requires_grad_(False)\n",
    "        \n",
    "        # Convert to device\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ConvNeXt model\n",
    "        x = self.convnext(x)\n",
    "        return x\n",
    "\n",
    "# Define instance of our ConvNeXt model\n",
    "model = ConvNeXtCNN(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1baae0",
   "metadata": {},
   "source": [
    "### ConvNeXt + SVM\n",
    "For our first situation, we will use SVM to do classification on our extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5eb6da-8330-44ec-96d5-ead70efe5b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     24\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(inputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:263\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 263\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/PIL/Image.py:3245\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3242\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3243\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3245\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3247\u001b[0m preinit()\n\u001b[1;32m   3249\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define our SVM classifier\n",
    "svm_classifier = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "\n",
    "# Construct results dict to track training\n",
    "results = {}\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train), 1):\n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    # Create data samplers for train and validation sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create data loaders for train and validation sets\n",
    "    train_loader = DataLoader(train, batch_size=32, sampler=train_sampler)\n",
    "    val_loader = DataLoader(train, batch_size=32, sampler=val_sampler)\n",
    "    \n",
    "    # Extract features and labels for the training set using ConvNeXt\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            train_features.append(outputs.cpu().numpy())\n",
    "            train_labels.append(labels.cpu().numpy())\n",
    "    train_features = np.concatenate(train_features)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "\n",
    "    # Train the SVM classifier\n",
    "    svm_classifier.fit(train_features, train_labels)\n",
    "    \n",
    "    # Extract features and labels for the validation set using ConvNeXt\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            val_features.append(outputs.cpu().numpy())\n",
    "            val_labels.append(labels.cpu().numpy())\n",
    "    val_features = np.concatenate(val_features)\n",
    "    val_labels = np.concatenate(val_labels)\n",
    "\n",
    "    # Evaluate the classifier on the validation set and extract metrics\n",
    "    val_predictions = svm_classifier.predict(val_features)\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "    results[fold] = accuracy\n",
    "    print(f'Fold {fold} Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print the average metrics across all folds\n",
    "average_accuracy = np.mean(list(results.values()))\n",
    "\n",
    "print(f'\\nK-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "for fold in results:\n",
    "    print(f'Fold {fold}: {results[fold]:.4f}')\n",
    "print(f'Average: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c0991-e26b-4c2c-85da-e4c7c60eb54c",
   "metadata": {},
   "source": [
    "## ConvNeXt + Softmax Classifier Training and Testing\n",
    "We will perform k-fold cross-validation testing on the Softmax classifier, which is trained the on features extracted by our ConvNeXt model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86ef1e2-752f-4e85-bedd-1e3508f3e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct results dict to track training\n",
    "results = {}\n",
    "\n",
    "# Create the softmax classifier pipeline via a Logistic Regression with Softmax activation.\n",
    "softmax_classifier = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=400, C=1.0, random_state=231)\n",
    ")\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train), 1):\n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    # Create data samplers for train and validation sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create data loaders for train and validation sets\n",
    "    train_loader = DataLoader(train, batch_size=32, sampler=train_sampler)\n",
    "    val_loader = DataLoader(train, batch_size=32, sampler=val_sampler)\n",
    "\n",
    "    # Extract features and labels for the training set using ConvNeXt\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    # Train the softmax classifier\n",
    "    softmax_classifier.fit(features, labels)\n",
    "\n",
    "    # Extract features and labels for the validation set using ConvNeXt\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            val_features.append(outputs.cpu().numpy())\n",
    "            val_labels.append(labels.cpu().numpy())\n",
    "    val_features = np.concatenate(val_features)\n",
    "    val_labels = np.concatenate(val_labels)\n",
    "\n",
    "    # Evaluate the classifier on the validation set and extract metrics\n",
    "    val_predictions = softmax_classifier.predict(val_features)\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "    results[fold] = accuracy\n",
    "    print(f'Fold {fold} Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print the average metrics across all folds\n",
    "average_accuracy = np.mean(list(results.values()))\n",
    "\n",
    "print(f'\\nK-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "for fold in results:\n",
    "    print(f'Fold {fold}: {results[fold]:.4f}')\n",
    "print(f'Average: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12377846-c63a-41c8-923d-3902e581801e",
   "metadata": {},
   "source": [
    "## ConvNeXt + PCA + SVM Classifier Training and Testing\n",
    "As an extension to our SVM implementation, the paper suggests that applying PCA on the resulting features derives higher accuracy before being loaded into the SVM classifier. We implement this approach below, performing k-fold cross-validation testing on the PCA + SVM classifier, which is trained the on features extracted by our ConvNeXt model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed75428c-079a-45ec-836f-7ef682d91427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 1 Accuracy: 0.8953\n",
      "Fold 2\n",
      "Fold 2 Accuracy: 0.8807\n",
      "Fold 3\n",
      "Fold 3 Accuracy: 0.8860\n",
      "Fold 4\n",
      "Fold 4 Accuracy: 0.8747\n",
      "Fold 5\n",
      "Fold 5 Accuracy: 0.8933\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 1: 0.8953\n",
      "Fold 2: 0.8807\n",
      "Fold 3: 0.8860\n",
      "Fold 4: 0.8747\n",
      "Fold 5: 0.8933\n",
      "Average: 0.8860\n"
     ]
    }
   ],
   "source": [
    "# Store the results of each fold\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=231)\n",
    "results = {}\n",
    "\n",
    "# Reduce dimensionality to 20 via PCA\n",
    "n_components = 20\n",
    "\n",
    "# Create the SVM classifier\n",
    "pca_classifier = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    PCA(n_components=n_components), \n",
    "    SVC(kernel='linear')\n",
    ")\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train), 1):\n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    # Create data samplers for train and validation sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create data loaders for train and validation sets\n",
    "    train_loader = DataLoader(train, batch_size=32, sampler=train_sampler)\n",
    "    val_loader = DataLoader(train, batch_size=32, sampler=val_sampler)\n",
    "    \n",
    "    # Extract features and labels for the training set\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            train_features.append(outputs.cpu().numpy())\n",
    "            train_labels.append(labels.cpu().numpy())\n",
    "    train_features = np.concatenate(train_features)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "\n",
    "    # Train the SVM classifier with PCA\n",
    "    pca_classifier.fit(train_features, train_labels)\n",
    "    \n",
    "    # Extract features and labels for the validation set\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            val_features.append(outputs.cpu().numpy())\n",
    "            val_labels.append(labels.cpu().numpy())\n",
    "    val_features = np.concatenate(val_features)\n",
    "    val_labels = np.concatenate(val_labels)\n",
    "\n",
    "    # Evaluate the classifier on the validation set\n",
    "    val_predictions = pca_classifier.predict(val_features)\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    results[fold] = accuracy\n",
    "    print(f'Fold {fold} Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print the average accuracy across all folds\n",
    "average_accuracy = np.mean(list(results.values()))\n",
    "print(f'\\nK-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "for fold in results:\n",
    "    print(f'Fold {fold}: {results[fold]:.4f}')\n",
    "print(f'Average: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048240c",
   "metadata": {},
   "source": [
    "### Testing and Metrics\n",
    "\n",
    "Now with our trained models, we will now test with our test set and store metrics for each model. The metrics that we will store are the following:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "\n",
    "The metrics are defined in our paper more clearly, but to calculate these we will calculate the the following values:\n",
    "- True Positive (TP)\n",
    "- False Positive (FP)\n",
    "- True Negative (TN)\n",
    "- False Negative (FN)\n",
    "\n",
    "We calculate these values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for training and validation sets\n",
    "def extract_features(loader, model):\n",
    "    features_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            features = model(inputs)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    return np.concatenate(features_list), np.concatenate(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89d2c4-5b77-4b2d-bd55-f7c4a54c0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features\n",
    "test_loader = DataLoader(test, batch_size=32, shuffle=False)\n",
    "test_features, test_labels = extract_features(test_loader, model)\n",
    "\n",
    "# List of trained classifiers\n",
    "classifiers = {\n",
    "    'SVM': svm_classifier,   # Assume svm_model is already trained\n",
    "    'Softmax': softmax_classifier,  # Another trained classifier\n",
    "    'SVM+PCA': pca_classifier   # Another trained classifier\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {class_name: {} for class_name in classifiers}\n",
    "\n",
    "# Evaluate each classifier\n",
    "for class_name, classifier in classifiers.items():\n",
    "    # Predict using the classifier\n",
    "    test_predictions = classifier.predict(test_features)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "    recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "    f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "    \n",
    "    # Store the results\n",
    "    results[class_name]['accuracy'] = accuracy\n",
    "    results[class_name]['precision'] = precision\n",
    "    results[class_name]['recall'] = recall\n",
    "    results[class_name]['f1'] = f1\n",
    "\n",
    "    # Print the results\n",
    "    print(f'{class_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "# Print a summary of the results\n",
    "print('Comparison of Classifiers on Test Set:')\n",
    "for clf_name in results:\n",
    "    print(f'{clf_name}: Accuracy={results[clf_name][\"accuracy\"]:.4f}, Precision={results[clf_name][\"precision\"]:.4f}, Recall={results[clf_name][\"recall\"]:.4f}, F1 Score={results[clf_name][\"f1\"]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
