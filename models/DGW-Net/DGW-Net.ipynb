{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (1.25.2)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (4.41.1)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (3.5.3)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: kaggle in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (1.6.14)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.4.1.post1)\n",
      "Collecting tensorflow (from -r ../../requirements.txt (line 8))\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 9)) (3.3.3)\n",
      "Collecting seaborn (from -r ../../requirements.txt (line 10))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers->-r ../../requirements.txt (line 2)) (4.66.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 4)) (4.12.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 4)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 4)) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (2024.2.2)\n",
      "Requirement already satisfied: python-slugify in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (1.26.9)\n",
      "Requirement already satisfied: bleach in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kaggle->-r ../../requirements.txt (line 6)) (6.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->-r ../../requirements.txt (line 7)) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->-r ../../requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->-r ../../requirements.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (3.11.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ml-dtypes~=0.3.1 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow->-r ../../requirements.txt (line 8)) (58.1.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Downloading grpcio-1.64.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras->-r ../../requirements.txt (line 9)) (13.7.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras->-r ../../requirements.txt (line 9)) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras->-r ../../requirements.txt (line 9)) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->-r ../../requirements.txt (line 11)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->-r ../../requirements.txt (line 11)) (2023.3)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers->-r ../../requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers->-r ../../requirements.txt (line 2)) (3.3)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow->-r ../../requirements.txt (line 8))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r ../../requirements.txt (line 8)) (2.1.2)\n",
      "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bleach->kaggle->-r ../../requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-slugify->kaggle->-r ../../requirements.txt (line 6)) (1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich->keras->-r ../../requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich->keras->-r ../../requirements.txt (line 9)) (2.14.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->-r ../../requirements.txt (line 9)) (0.1.2)\n",
      "Downloading tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl (227.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.0/227.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.64.0-cp310-cp310-macosx_12_0_universal2.whl (10.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-macosx_10_9_universal2.whl (389 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wrapt, wheel, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, ml-dtypes, markdown, grpcio, google-pasta, gast, tensorboard, astunparse, seaborn, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.0\n",
      "    Uninstalling ml-dtypes-0.4.0:\n",
      "      Successfully uninstalled ml-dtypes-0.4.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.64.0 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 opt-einsum-3.3.0 protobuf-4.25.3 seaborn-0.13.2 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 wheel-0.43.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "# Initial setup cell\n",
    "!pip3 install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup\n",
    "Initializes our train and test set and stores the ground truth of each image. We use Torch libraries to handle this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data setup dependencies\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below extracts images from our dataset, resizes each into a fourth their original size (768 -> 192), and converts them into Torch tensors. The ImageFolder class allows us to lazyload our images to preserve our computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to our lung_image_sets\n",
    "data_dir = \"../../lung_colon_image_set/lung_image_sets\"\n",
    "\n",
    "# Set the resize size of the images\n",
    "resized_size = 80\n",
    "\n",
    "# Convert images into Tensors\n",
    "tensor_data = transforms.Compose([\n",
    "  transforms.Resize((resized_size, resized_size)),   # Cut image into a fourth of original size\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "data = ImageFolder(root=data_dir, transform=tensor_data)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train, test = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "load_train = DataLoader(train, batch_size=32, shuffle=True)\n",
    "load_test = DataLoader(test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGW-Net: A basic CNN image classifier\n",
    "\n",
    "DGW-Net (abbreviated from Dino-Garcia-Wang Net) is a simple CNN architecture that follows the structure below:\n",
    "- Input -> CONV (3x3, 64 filters) -> ReLU -> MaxPool -> CONV (3x3, 64 filters) -> ReLU -> MaxPool -> FC\n",
    "\n",
    "This model architecture follows from our simple CNN implementation for CIFAR-10 on Assignment 2. We decided on 64 filters as empirically this number has proven to be successful in models like ResNet. The implementation follows below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our DGW-Net Architecture\n",
    "class DGWNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DGWNet, self).__init__()\n",
    "        \n",
    "        # First convolutional block: 64 filters used. (L, L, 3) -> (L//2, L//2, 64)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolutional block: 64 filters used. (L//2, L//2, 64) -> (L//4, L//4, 64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # FC Layer -> 128 Features\n",
    "        self.fc = nn.Linear(resized_size // 4 * resized_size // 4 * 64, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten the output from the convolutional blocks\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and Constants\n",
    "Here we define our hyperparameters and constants that will stay constant throughout all implementations of DGW-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "learning_rate = 5e-4\n",
    "momentum = 0.9\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Define number of folds\n",
    "k_folds = 5\n",
    "\n",
    "# Create a KFold object with 5 splits\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=231)\n",
    "folds = kfold.split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGW-Net + SVM\n",
    "For our first situation, we will use SVM to do classification on our extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGWSVM(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(DGWSVM, self).__init__()\n",
    "      \n",
    "      # Base DGWNet feature extractor -> 128 features\n",
    "      self.extract_features = DGWNet()\n",
    "      \n",
    "      # SVM layer\n",
    "      self.svm = nn.Linear(128, 3) \n",
    "  \n",
    "  def forward(self, x):\n",
    "        # Pass the input through the base CNN\n",
    "        x = self.extract_features(x)\n",
    "        \n",
    "        # SVM layer\n",
    "        x = self.svm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instance of our model\n",
    "svm_model = DGWSVM()\n",
    "\n",
    "# Define our loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "svm_optimizer = optim.SGD(svm_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Check current device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our DGW-SVM model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over folds in training\n",
    "for fold, (train_indices, val_indices) in enumerate(folds, 1):\n",
    "    # Create data samplers for train and validation sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create data loaders for train and validation sets\n",
    "    train_loader = DataLoader(data, batch_size=32, sampler=train_sampler)\n",
    "    val_loader = DataLoader(data, batch_size=32, sampler=val_sampler)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in load_train:\n",
    "            # Move the input data to the device (CPU or GPU)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            svm_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = svm_model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            svm_optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print the average loss for the epoch\n",
    "        epoch_loss = running_loss / len(load_train)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGW-Net + Softmax\n",
    "For our second situation, we will use Softmax to do classification on our extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGWSoftmax(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(DGWSoftmax, self).__init__()\n",
    "      \n",
    "      # Base DGWNet feature extractor -> 128 features\n",
    "      self.extract_features = DGWNet()\n",
    "      \n",
    "      # Pre-softmax FC layer\n",
    "      self.fc = nn.Linear(128, 3) \n",
    "  \n",
    "  def forward(self, x):\n",
    "        # Pass the input through the base CNN\n",
    "        x = self.extract_features(x)\n",
    "        \n",
    "        # Softmax layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # Apply softmax activation\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instance of our model\n",
    "softmax_model = DGWSoftmax()\n",
    "\n",
    "# Define our optimizer (reuse same loss function from before)\n",
    "softmax_optimizer = optim.SGD(softmax_model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our DGW-Softmax model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over folds in training\n",
    "for fold, (train_indices, val_indices) in enumerate(folds, 1):\n",
    "    # Create data samplers for train and validation sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create data loaders for train and validation sets\n",
    "    train_loader = DataLoader(data, batch_size=32, sampler=train_sampler)\n",
    "    val_loader = DataLoader(data, batch_size=32, sampler=val_sampler)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in load_train:\n",
    "            # Move the input data to the device (CPU or GPU)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            softmax_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = softmax_model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            softmax_optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print the average loss for the epoch\n",
    "        epoch_loss = running_loss / len(load_train)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
